name: Fetch and Clean RSS Feed

on:
  schedule:
    - cron: '0 */6 * * *'  # every 6 hours
  workflow_dispatch:

jobs:
  fetch:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          pip install lxml requests beautifulsoup4

      - name: Fetch and Clean RSS Feed
        run: |
          python3 <<'EOF'
          import requests
          from lxml import etree
          from bs4 import BeautifulSoup
          from datetime import datetime, timezone
          import re

          print("Fetching original feed…")
          # Source feed from rss.app
          response = requests.get('https://rss.app/feeds/PnXLTXpDQSdLv1cb.xml')
          root = etree.fromstring(response.content)
          channel = root.find('channel')
          items = channel.findall('item')[:8]

          nsmap = {
              'content': 'http://purl.org/rss/1.0/modules/content/',
              'wfw': 'http://wellformedweb.org/CommentAPI/',
              'dc': 'http://purl.org/dc/elements/1.1/',
              'atom': 'http://www.w3.org/2005/Atom',
              'sy': 'http://purl.org/rss/1.0/modules/syndication/',
              'slash': 'http://purl.org/rss/1.0/modules/slash/',
              'media': 'http://search.yahoo.com/mrss/',
              'snf': 'http://www.smartnews.be/snf'
          }
          for prefix, uri in nsmap.items():
              etree.register_namespace(prefix, uri)

          # Use your provided logo URL
          logo_url = 'https://i.ibb.co/sptKgp34/CTV-Feed-Logo.png'
          fallback_thumbnail = logo_url  # Use logo as fallback thumbnail too

          xml_parts = []
          xml_parts.append('<?xml version="1.0" encoding="UTF-8"?>')
          xml_parts.append('<rss version="2.0"')
          xml_parts.append(' xmlns:content="http://purl.org/rss/1.0/modules/content/"')
          xml_parts.append(' xmlns:wfw="http://wellformedweb.org/CommentAPI/"')
          xml_parts.append(' xmlns:dc="http://purl.org/dc/elements/1.1/"')
          xml_parts.append(' xmlns:atom="http://www.w3.org/2005/Atom"')
          xml_parts.append(' xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"')
          xml_parts.append(' xmlns:slash="http://purl.org/rss/1.0/modules/slash/"')
          xml_parts.append(' xmlns:media="http://search.yahoo.com/mrss/"')
          xml_parts.append(' xmlns:snf="http://www.smartnews.be/snf">')
          xml_parts.append('<channel>')

          # Insert SmartNews logo
          xml_parts.append(f'<snf:logo><url>{logo_url}</url></snf:logo>')
          print(f"✓ Added logo at start of channel: {logo_url}")

          # Copy other channel elements (skip any existing logos)
          for elem in channel:
              if elem.tag == 'item':
                  break
              if 'logo' not in elem.tag.lower():
                  xml_parts.append(etree.tostring(elem, encoding='unicode', method='xml'))

          cdata_count = 0

          def fetch_featured_image(article_url):
              try:
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (compatible; SmartFeed-Builder/3.0)',
                      'Accept': 'text/html'
                  }
                  res = requests.get(article_url, headers=headers, timeout=10)
                  if not res.ok:
                      return None
                  html = res.text
                  candidates = []
                  # og:image
                  candidates += re.findall(r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
                  # twitter:image
                  candidates += re.findall(r'<meta[^>]+name=["\']twitter:image["\'][^>]+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
                  # article:image
                  candidates += re.findall(r'<meta[^>]+property=["\']article:image["\'][^>]+content=["\']([^"\']+)["\']', html, re.IGNORECASE)
                  for img_url in candidates:
                      fname = img_url.lower().split('/')[-1]
                      if 'logo' in fname or 'icon' in fname:
                          continue
                      if any(sz in img_url.lower() for sz in ['150x150', '100x100', '50x50']):
                          continue
                      if 'watch-' in fname or fname.endswith('.webp'):
                          return img_url
                      return img_url
                  if candidates:
                      return candidates[0]
                  return None
              except Exception as e:
                  print(f"    ! Error fetching image: {e}")
                  return None

          for idx, item in enumerate(items):
              title_elem = item.find('title')
              title_text = title_elem.text if title_elem is not None else 'Unknown'
              print(f"\nItem {idx+1}: {title_text[:50]}")

              xml_parts.append('<item>')

              original_content = None
              description_content = None
              article_link = None

              # Extract existing content and description
              for elem in item:
                  tag_name = etree.QName(elem.tag).localname if '}' in elem.tag else elem.tag
                  namespace = etree.QName(elem.tag).namespace if '}' in elem.tag else None
                  if tag_name == 'link':
                      article_link = elem.text.split('?')[0] if elem.text else None
                  if namespace == nsmap['content'] and tag_name == 'encoded':
                      original_content = elem.text or ''
                  if tag_name == 'description':
                      description_content = elem.text or ''

              # Copy elements except thumbnails and content:encoded
              for elem in item:
                  tag_name = etree.QName(elem.tag).localname if '}' in elem.tag else elem.tag
                  namespace = etree.QName(elem.tag).namespace if '}' in elem.tag else None
                  if namespace == nsmap['media']:
                      continue  # skip existing media:thumbnail
                  if namespace == nsmap['content'] and tag_name == 'encoded':
                      continue  # skip existing content:encoded
                  xml_parts.append(etree.tostring(elem, encoding='unicode', method='xml'))

              # Decide what goes into content:encoded
              html_content = original_content or description_content or ''
              if html_content:
                  soup = BeautifulSoup(html_content, 'html.parser')
                  links = soup.find_all('a')
                  if len(links) > 3:
                      for link in links[3:]:
                          link.unwrap()
                  clean_html = str(soup)
                  xml_parts.append(f'<content:encoded><![CDATA[{clean_html}]]></content:encoded>')
                  cdata_count += 1
              else:
                  xml_parts.append('<content:encoded><![CDATA[]]></content:encoded>')

              # Ensure dc:creator and pubDate if missing
              if item.find('dc:creator', nsmap) is None:
                  xml_parts.append('<dc:creator>CableTV.com</dc:creator>')
              if item.find('pubDate') is None:
                  xml_parts.append(
                      f'<pubDate>{datetime.now(timezone.utc).strftime("%a, %d %b %Y %H:%M:%S +0000")}</pubDate>'
                  )

              # Determine a thumbnail
              featured_image = None
              if article_link:
                  featured_image = fetch_featured_image(article_link)
              if not featured_image:
                  featured_image = fallback_thumbnail
              # fix relative URLs
              if not featured_image.startswith('http'):
                  if featured_image.startswith('//'):
                      featured_image = 'https:' + featured_image
                  elif featured_image.startswith('/'):
                      featured_image = 'https://www.cabletv.com' + featured_image
              featured_image = featured_image.replace('&', '&amp;')
              xml_parts.append(f'<media:thumbnail url="{featured_image}"/>')
              xml_parts.append('</item>')

          xml_parts.append('</channel></rss>')
          # Replace the source feed URL with your GitHub Pages feed URL in the self-link (if present)
          xml_string = ''.join(xml_parts).replace(
              'https://rss.app/feeds/PnXLTXpDQSdLv1cb.xml',
              'https://watchctv.github.io/SmartNews/feed.xml'
          )

          with open('feed.xml', 'w', encoding='utf-8') as f:
              f.write(xml_string)

          print(f"\n✓ Feed created with {cdata_count} CDATA sections")
          print(f"✓ Output size: {len(xml_string):,} bytes")
          EOF

      - name: Verify logo in feed
        run: |
          echo "=== Checking for logo in feed ==="
          if grep -q '<snf:logo>' feed.xml; then
            echo "✓ Logo found"
          else
            echo "✗ Logo NOT found"
          fi
          echo ""
          echo "=== Logo content ==="
          grep -A1 '<snf:logo>' feed.xml || echo "No logo to display"

      - name: Check for changes
        id: changes
        run: |
          git diff --quiet feed.xml || echo "changed=true" >> "$GITHUB_OUTPUT"

      - name: Commit and push feed
        if: steps.changes.outputs.changed == 'true'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add feed.xml
          git commit -m "Update RSS feed $(date '+%Y-%m-%d %H:%M:%S')"
          git pull --rebase origin main
          git push origin main
